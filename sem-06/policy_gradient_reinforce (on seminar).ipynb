{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEiif3dBXnec"
   },
   "source": [
    "# Policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSqmnob8Xneh"
   },
   "source": [
    "## Алгоритм Reinforce\n",
    "В некоторых задачах для нахождения удовлетворяющей стратегии необязательно изучать структуру всей среды. Например, в задаче поднятия кубика робототехнической рукой вместо точной аппроксимации $Q(s,a)$ достаточно знать, что выгоднее двигаться вправо, если кубик справа, и влево в ином случае. Алгоритм Reinforce (Monte Carlo policy gradient) - это алгоритм поиска стратегий, в котором параметры, задающие стохастическую стратегию, изменяются в соответствии с градиентом математического ожидания награды: \n",
    "\n",
    "$$J(\\theta)=\\mathbb E_{\\tau\\sim p_{\\theta}(\\tau)}(\\sum_t\\gamma^tr(s_t,a_t))$$\n",
    "\n",
    "$$\\theta\\leftarrow\\theta+\\alpha\\nabla_{\\theta}J(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EYi48s1pXnek"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1551346194660,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "HgBcqS7dXnes",
    "outputId": "e57593ed-1766-4831-91eb-99acb84ad43a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Box(4,)\n",
      "Action Space Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Будем обучать агента, используя сначала простое окружение CartPole-v0,\n",
    "# а затем усложним задачу взяв Acrobot-v1\n",
    "\n",
    "# Создаем окружение \n",
    "# Здесь ваш код\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "gamma = 0.95\n",
    "\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Action Space\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJj1uA4JXney"
   },
   "source": [
    "### Задание 1. Определяем архитектуру сети\n",
    "Стратегия задается весами нейронной сети и является стохастической, т.е. в состоянии $s$ она представляет себой некоторое распределение $\\pi_\\theta(s)$, поэтому на вход сети будет подаваться состояние $s$, а на выходе будут вероятности действий.\n",
    "\n",
    "#### Задаем входы сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pTym-T3XXne0"
   },
   "outputs": [],
   "source": [
    "# Задаем переменные, которые будут подаваться на вход нейронной сети\n",
    "\n",
    "# Состояния\n",
    "# Здесь ваш код\n",
    "observations = tf.placeholder(tf.float32, [None, *observation_shape], name=\"observations\")\n",
    "# Совершенные действия\n",
    "# Здесь ваш код\n",
    "actions = tf.placeholder(tf.int32, [None, n_actions], name=\"actions\")\n",
    "\n",
    "# Вознаграждение\n",
    "# Здесь ваш код\n",
    "discounted_episode_rewards = tf.placeholder(tf.float32, [None], name=\"discounted_episode_rewards\")\n",
    "\n",
    "all_inputs = [observations, actions, discounted_episode_rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOLMwQLBXne5"
   },
   "source": [
    "#### Определяем граф вычислений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 592,
     "status": "ok",
     "timestamp": 1551347505109,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "SoxRasDjXne7",
    "outputId": "82f6902e-6f8d-4466-cf74-07ea9845de88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Задаем внутренние и выходной слои нейронной сети\n",
    "# Здесь ваш код\n",
    "nn1 = tf.contrib.layers.fully_connected(inputs=observations, num_outputs=50, activation_fn=tf.nn.relu, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "nn2 = tf.contrib.layers.fully_connected(inputs=nn1, num_outputs=20, activation_fn=tf.nn.relu, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "nn3 = tf.contrib.layers.fully_connected(inputs=nn2, num_outputs=n_actions, activation_fn=None, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "  \n",
    "\n",
    "probs_out = tf.nn.softmax(nn3) \n",
    "# Выход последнего слоя преобразуется в стохастическую стратегию, \n",
    "# поэтому количество нейронов должно быть равно n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPp4JLebXnfA"
   },
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    \n",
    "    # Считаем дисконтированное вознаграждение \"G = r + gamma*r' + gamma^2*r'' + ...\"\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    # Нормализуем данные\n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucvs9pCIXnfG"
   },
   "source": [
    "### Задание 2. Функция потерь\n",
    "\n",
    "Теперь определим функцию потерь (Crossentropy loss)\n",
    "\n",
    "Градиент стратегии выглядит следующим образом $\\nabla_{\\theta}J_{\\theta}\\approx\\frac{1}{N}\\sum_{i=1}^N\\sum_{t=1}^T\\nabla_{\\theta}log\\pi_{\\theta}(a_{i,t}|s_{i,t})R $. Чтобы автоматически вычислить градиент, необходимо задать граф, который имеет градиент такого же вида. Для этого используется \"псевдо\" функция потерь: $$\\tilde{J}_{\\theta}=\\frac{1}{N}\\sum_{i=1}^N\\sum_{t=1}^Tlog\\pi_{\\theta}(a_{i,t}|s_{i,t})R $$.\n",
    "\n",
    "Подробнее http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html#lecture-videos (6 sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1551348147089,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "G5u4N-vlXnfI",
    "outputId": "eb9d522c-7b17-4794-ef0d-d811402ded20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-cd2bdbee30f7>:10: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n"
     ]
    }
   ],
   "source": [
    "# Применяем кросс энтропию к ПОСЛЕДНЕМУ слою сети tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "# Здесь ваш код\n",
    "neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=nn3, labels=actions)\n",
    "\n",
    "# Умножаем на дисконтированное вознаграждение и берем среднее, чтобы получить искомую функцию потерь\n",
    "# Здесь ваш код\n",
    "loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards)     \n",
    "\n",
    "# Задаем метод оптимизации, который будем использовать (например adam с lr 0.01)\n",
    "# Здесь ваш код\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.01)\n",
    "train_op = optimizer.minimize(loss, global_step=tf.contrib.framework.get_global_step())\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjbd6DaXXnfP"
   },
   "source": [
    "Градиент имеет большую дисперсию. Для ее уменьшения можно использовать следующую функцию потерь: $$\\tilde{J}_{\\theta}=\\frac{1}{N}\\sum_{i=1}^N\\sum_{t=1}^Tlog\\pi_{\\theta}(a_{i,t}|s_{i,t})(R-b) $$.\n",
    "\n",
    "$b$ - может быть константой (это не сильно улучшит работу алгоритма), а может быть функцией от $s$, например $V(s)$. $V(s)$ может быть аппроксимированна другой нейронной сетью. Настройка может проходить по методу наименьших квадратов: необходимо задать функцию ошибок и можно добавить ее к loss'у с некоторым коэффициентом или минимизировать отдельно. Пример: https://github.com/yrlu/reinforcement_learning/blob/master/policy_gradient/reinforce_w_baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3757
    },
    "colab_type": "code",
    "id": "1V7qmEw9XnfS",
    "outputId": "18d3a370-ca29-4d59-a4af-100d578338e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  0\n",
      "Reward:  14.0\n",
      "Mean Reward 14.0\n",
      "Max reward so far:  14.0\n",
      "==========================================\n",
      "Episode:  50\n",
      "Reward:  200.0\n",
      "Mean Reward 109.17647058823529\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  100\n",
      "Reward:  35.0\n",
      "Mean Reward 125.29702970297029\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  150\n",
      "Reward:  119.0\n",
      "Mean Reward 140.71523178807948\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  200\n",
      "Reward:  200.0\n",
      "Mean Reward 145.33830845771143\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  250\n",
      "Reward:  200.0\n",
      "Mean Reward 156.22709163346613\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  300\n",
      "Reward:  200.0\n",
      "Mean Reward 157.77408637873754\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  350\n",
      "Reward:  200.0\n",
      "Mean Reward 163.7891737891738\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  400\n",
      "Reward:  200.0\n",
      "Mean Reward 168.30423940149626\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  450\n",
      "Reward:  200.0\n",
      "Mean Reward 169.549889135255\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  500\n",
      "Reward:  105.0\n",
      "Mean Reward 167.1996007984032\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  550\n",
      "Reward:  200.0\n",
      "Mean Reward 168.32486388384754\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  600\n",
      "Reward:  200.0\n",
      "Mean Reward 170.96006655574044\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  650\n",
      "Reward:  89.0\n",
      "Mean Reward 171.01382488479263\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  700\n",
      "Reward:  46.0\n",
      "Mean Reward 164.30099857346647\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  750\n",
      "Reward:  118.0\n",
      "Mean Reward 160.60719041278296\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  800\n",
      "Reward:  159.0\n",
      "Mean Reward 156.82771535580525\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  850\n",
      "Reward:  104.0\n",
      "Mean Reward 154.74618096357227\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  900\n",
      "Reward:  190.0\n",
      "Mean Reward 155.61931187569368\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  950\n",
      "Reward:  200.0\n",
      "Mean Reward 157.68980021030495\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1000\n",
      "Reward:  177.0\n",
      "Mean Reward 159.6113886113886\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1050\n",
      "Reward:  200.0\n",
      "Mean Reward 161.51950523311132\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1100\n",
      "Reward:  200.0\n",
      "Mean Reward 163.2615803814714\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1150\n",
      "Reward:  200.0\n",
      "Mean Reward 164.85751520417028\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1200\n",
      "Reward:  200.0\n",
      "Mean Reward 166.32056619483762\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1250\n",
      "Reward:  200.0\n",
      "Mean Reward 167.66666666666666\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1300\n",
      "Reward:  200.0\n",
      "Mean Reward 168.90930053804766\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1350\n",
      "Reward:  200.0\n",
      "Mean Reward 170.059955588453\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1400\n",
      "Reward:  200.0\n",
      "Mean Reward 171.12847965738757\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1450\n",
      "Reward:  200.0\n",
      "Mean Reward 172.12336319779462\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1500\n",
      "Reward:  200.0\n",
      "Mean Reward 173.05196535642904\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1550\n",
      "Reward:  200.0\n",
      "Mean Reward 173.92069632495165\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1600\n",
      "Reward:  200.0\n",
      "Mean Reward 174.73516552154902\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1650\n",
      "Reward:  200.0\n",
      "Mean Reward 175.50030284675955\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1700\n",
      "Reward:  200.0\n",
      "Mean Reward 174.80305702527926\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1750\n",
      "Reward:  200.0\n",
      "Mean Reward 175.52255853797828\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1800\n",
      "Reward:  200.0\n",
      "Mean Reward 176.20210993892283\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1850\n",
      "Reward:  200.0\n",
      "Mean Reward 176.09562398703403\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1900\n",
      "Reward:  200.0\n",
      "Mean Reward 176.72435560231457\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  1950\n",
      "Reward:  200.0\n",
      "Mean Reward 177.3208610968734\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  2000\n",
      "Reward:  200.0\n",
      "Mean Reward 177.88755622188904\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  2050\n",
      "Reward:  200.0\n",
      "Mean Reward 178.42662116040955\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  2100\n",
      "Reward:  200.0\n",
      "Mean Reward 178.9400285578296\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  2150\n",
      "Reward:  200.0\n",
      "Mean Reward 179.42956764295675\n",
      "Max reward so far:  200.0\n"
     ]
    }
   ],
   "source": [
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "max_episodes = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "        state = env.reset()\n",
    "        # Можно посмотреть как выглядит окружение, если мы не используем google colab\n",
    "#         env.render()\n",
    "           \n",
    "        while True:\n",
    "            \n",
    "            # Получаем распределение вероятностей действий в соответствии со стохастической стратегией агента\n",
    "            # Здесь ваш код\n",
    "            probs = sess.run(probs_out, feed_dict = {\n",
    "                observations : state.reshape([1, env.observation_space.shape[0]])})\n",
    "            \n",
    "            # Выбираем действие (согласно распределению)\n",
    "            # Здесь ваш код\n",
    "            action =  np.random.choice(range(probs.shape[1]), p=probs.ravel())\n",
    "            \n",
    "            # Применяем действие в среде \n",
    "            # Здесь ваш код\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            episode_states.append(state)\n",
    "            action_ = np.zeros(n_actions)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                \n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                if episode % 50 == 0:\n",
    "                    print(\"==========================================\")\n",
    "                    print(\"Episode: \", episode)\n",
    "                    print(\"Reward: \", episode_rewards_sum)\n",
    "                    print(\"Mean Reward\", mean_reward)\n",
    "                    print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                # Считаем дисконтированное вознаграждение\n",
    "                # Здесь ваш код\n",
    "                discounted_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                                \n",
    "                # Считаем значение функции потерь и настраиваем веса сети при помощи оптимизатора\n",
    "                # Здесь ваш код\n",
    "                loss_, _ = sess.run([loss, train_op], feed_dict={\n",
    "                    observations: np.vstack(np.array(episode_states)),\n",
    "                    actions: np.vstack(np.array(episode_actions)), \n",
    "                    discounted_episode_rewards: discounted_rewards\n",
    "                })\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQ9rKa5_Xnfa"
   },
   "source": [
    "### Задание 3. Проводим эксперименты.\n",
    "\n",
    "Постройте график получаемого суммарного вознаграждения от номера эпизода.\n",
    "\n",
    "Сделайте тоже самое для задачи CartPole-v0. \n",
    "\n",
    "Усложните архитектуру сети. Улучшает ли это производительность?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55JVSBNNXnfc"
   },
   "source": [
    "### Дополнительные материалы\n",
    "\n",
    "Пример алгоритма DDPG (https://arxiv.org/pdf/1509.02971.pdf) - алгоритм использующий градиент стратегий и позволяющий работать с непрерывным множеством действий:\n",
    "\n",
    "https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of policy_gradient_reinforce.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/Tviskaron/IDA2-2019/blob/master/sem-06/policy_gradient_reinforce.ipynb",
     "timestamp": 1551349759181
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
