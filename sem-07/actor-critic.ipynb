{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 7: Actor-Critic\n",
    "\n",
    "\n",
    "## Майнор ВШЭ, 7.03.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом семинаре будем рассматривать простой эксперимент с алгоритмом Actor-Critic.\n",
    "\n",
    "Рассмотрим задачу с маятиником - здесь количество действий не ограничено!\n",
    "\n",
    "![Маятник](https://cdn-images-1.medium.com/max/1600/1*J_oEx0kpBpwXoVmRytn6qg.gif)\n",
    "\n",
    "Здесь невозможно правильно определить функцию $Q(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Модель Actor-Critic](https://cdn-images-1.medium.com/max/1600/1*-GfRVLWhcuSYhG25rN0IbA.png)\n",
    "\n",
    "\n",
    "Модель актор-критик имеет два отдельных аппроксиматора: первый используется для предсказания того, какое действие предпринять, учитывая текущее состояние среды, и второй - чтобы найти значение полезности действия/состояния."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Модель актора\n",
    "\n",
    "Начнем с импортирования всех необходимых библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестировать нашу модель будем на простой задаче с перевернутым маятником."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Создаем окружение Pendulum - смотрим как оно выглядит\n",
    "env = gym.make(\"Pendulum-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с определения модели актера. Цель модели состоит в том, чтобы, учитывая текущее состояние среды, определить наилучшее действие. Для начала будем использовать несколько полносвязных слоев, отображающих наблюдение среды в точку в пространстве среды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_actor_model(env):\n",
    "    state_input = Input(shape=env.observation_space.shape)\n",
    "    h1 = Dense(24, activation='relu')(state_input)\n",
    "    h2 = Dense(48, activation='relu')(h1)\n",
    "    h3 = Dense(24, activation='relu')(h2)\n",
    "    output = Dense(env.action_space.shape[0], activation='relu')(h3)\n",
    "\n",
    "    model = Model(input=state_input, output=output)\n",
    "    adam  = Adam(lr=0.001)\n",
    "    model.compile(loss=\"mse\", optimizer=adam)\n",
    "    return state_input, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы хотим определить, какое изменение параметров (в модели актора) приведет к наибольшему увеличению значения Q (предсказанному моделью критика)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "actor_state_input, actor_model = create_actor_model(env)\n",
    "_, target_actor_model = create_actor_model(env)\n",
    "\n",
    "actor_critic_grad = tf.placeholder(tf.float32, \n",
    "    [None, env.action_space.shape[0]]) # where we will feed de/dC (from critic)\n",
    "\n",
    "actor_model_weights = actor_model.trainable_weights\n",
    "actor_grads = tf.gradients(actor_model.output, \n",
    "    actor_model_weights, -actor_critic_grad) # dC/dA (from actor)\n",
    "grads = zip(actor_grads, actor_model_weights)\n",
    "optimize = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Модель критика\n",
    "\n",
    "Критик нужен для того, чтобы по состоянию окружающей среды и совершенному действию в качестве входных данных рассчитать соответствующую оценку. Построим его модель с помощью нескольких полвносвязных слоев с одним объединением перед окончательным предсказанием Q-значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_critic_model(env):\n",
    "    state_input = Input(shape=env.observation_space.shape)\n",
    "    state_h1 = Dense(24, activation='relu')(state_input)\n",
    "    state_h2 = Dense(48)(state_h1)\n",
    "\n",
    "    action_input = Input(shape=env.action_space.shape)\n",
    "    action_h1    = Dense(48)(action_input)\n",
    "\n",
    "    merged    = Add()([state_h2, action_h1])\n",
    "    merged_h1 = Dense(24, activation='relu')(merged)\n",
    "    output = Dense(1, activation='relu')(merged_h1)\n",
    "    model  = Model(input=[state_input,action_input], output=output)\n",
    "\n",
    "    adam  = Adam(lr=0.001)\n",
    "    model.compile(loss=\"mse\", optimizer=adam)\n",
    "    return state_input, action_input, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужны ссылки как на входы действий, так и на входы по состоянию, поскольку нам нужно использовать их при обновлении для сети актора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "critic_state_input, critic_action_input, critic_model = create_critic_model(env)\n",
    "_, _, target_critic_model = create_critic_model(env)\n",
    "\n",
    "critic_grads = tf.gradients(critic_model.output, critic_action_input) # where we calcaulte de/dC for feeding above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "# Initialize for later gradient calculations\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3. Определяем метод обучения модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь определим метод обучения по памяти. Мы просто находим дисконтированное будущее вознаграждение и обучаемся на этом. Обучение проводим на паре состояние/действие и используем target_critic_model для прогнозирования будущей награды. Для актора мы уже определили как градиенты будут работать в сети, и теперь просто должны вызвать их с действиями и состояниями, которые берем из памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .95\n",
    "\n",
    "def train(sess, memory):\n",
    "    batch_size = 32\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    rewards = []\n",
    "    samples = random.sample(memory, batch_size)\n",
    "  \n",
    "    \n",
    "    for sample in samples:\n",
    "        cur_state, action, reward, new_state, _ = sample\n",
    "        predicted_action = actor_model.predict(cur_state)\n",
    "        grads = sess.run(critic_grads, feed_dict={\n",
    "            critic_state_input:  cur_state,\n",
    "            critic_action_input: predicted_action\n",
    "        })[0]\n",
    "\n",
    "        sess.run(optimize, feed_dict={\n",
    "            actor_state_input: cur_state,\n",
    "            actor_critic_grad: grads\n",
    "        })\n",
    "        \n",
    "    for sample in samples:\n",
    "        cur_state, action, reward, new_state, done = sample\n",
    "        if not done:\n",
    "            target_action = target_actor_model.predict(new_state)\n",
    "            future_reward = target_critic_model.predict(\n",
    "                [new_state, target_action])[0][0]\n",
    "            reward += gamma * future_reward\n",
    "        critic_model.fit([cur_state, action], reward, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой модели мы используем целевую сеть для улучшения сходимости. Мы должны обновлять ее веса на каждом временном шаге. Однако мы делаем это с меньшей частотой ,которая задается параметром $\\tau$. Проделываем эту операцию как для актера так и для критика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau   = .125\n",
    "\n",
    "def update_target():\n",
    "    actor_model_weights  = actor_model.get_weights()\n",
    "    actor_target_weights = target_actor_model.get_weights()\n",
    "\n",
    "    for i in range(len(actor_target_weights)):\n",
    "        actor_target_weights[i] = (1-tau)*actor_model_weights[i]+tau*actor_target_weights[i]\n",
    "    target_actor_model.set_weights(actor_target_weights)\n",
    "\n",
    "\n",
    "    critic_model_weights  = critic_model.get_weights()\n",
    "    critic_target_weights = target_critic_model.get_weights()\n",
    "\n",
    "    for i in range(len(critic_target_weights)):\n",
    "        critic_target_weights[i] = (1-tau)*critic_model_weights[i]+tau*critic_target_weights[i]\n",
    "    target_critic_model.set_weights(critic_target_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Проводим эксперименты.\n",
    "\n",
    "Записываем полный цикл обучения: запоминиаем статистику и проводим на ней обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3edca05105ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5450ed005b39>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, memory)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpredicted_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         grads = sess.run(critic_grads, feed_dict={\n\u001b[1;32m     15\u001b[0m             \u001b[0mcritic_state_input\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_trials = 10000\n",
    "trial_len  = 500\n",
    "epsilon = 1.0\n",
    "epsilon_decay = .995\n",
    "\n",
    "cur_state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "while True:\n",
    "    env.render()\n",
    "    cur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
    "    \n",
    "    epsilon *= epsilon_decay\n",
    "    ### Реализуем epsilon-жадную стратегию\n",
    "    if np.random.random() < epsilon:\n",
    "        action =  env.action_space.sample()\n",
    "    else:\n",
    "        action = actor_model.predict(cur_state)\n",
    "    \n",
    "    \n",
    "    action = action.reshape((1, env.action_space.shape[0]))\n",
    "\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "    ### Сохраняем в память данные\n",
    "    memory.append([cur_state, action, reward, new_state, done])\n",
    "    \n",
    "    train(sess, memory)\n",
    "    \n",
    "    update_target()\n",
    "\n",
    "    cur_state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте график вознаграждения в зависимости от номера эпизода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
