{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CpceefzmsN45"
   },
   "source": [
    "## Семинар 2: Динамическое программирование\n",
    "### Майнор ВШЭ, 24.01.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uX-Ky982sN47"
   },
   "source": [
    "Рассмотрим алгоритм итерации по оценкам состояниям $V$ (Value Iteration):\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "На основе оценки $V_i$ можно посчитать функцию оценки $Q_i$ действия $a$ в состоянии $s$:\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "\n",
    "$$V_{(i+1)}(s) = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1uofVIWsN48"
   },
   "source": [
    "Зададим напрямую модель MDP с картинки:\n",
    "\n",
    "<img src=\"https://github.com/Tviskaron/IDA2-2019/blob/master/sem-02/mdp.png?raw=1\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ebF4HaPsN48"
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's1': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "try:\n",
    "  from mdp import MDP\n",
    "except ModuleNotFoundError:\n",
    "  !wget -nc https://raw.githubusercontent.com/Tviskaron/IDA2-2019/master/sem-02/mdp.py\n",
    "  from mdp import MDP\n",
    "import numpy as np\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "0bcgBOZbsN5A",
    "outputId": "9ff86175-e1cd-4d8b-d5ec-72f1cfb23cd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.get_all_states = ('s2', 's0', 's1')\n",
      "mdp.get_possible_actions('s1') =  ('a0', 'a1')\n",
      "mdp.get_next_states('s1', 'a0') =  {'s2': 0.2, 's0': 0.7, 's1': 0.1}\n",
      "mdp.get_reward('s1', 'a0', 's0') =  5\n",
      "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYDjsDv2sN5F"
   },
   "source": [
    "### Задание 1\n",
    "\n",
    "Реализуем итерационное вычисление функций $V$ и $Q$ и применим их для заданого вручную MDP.\n",
    "\n",
    "Вначале вычисляем оценку состояния-действия:\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVSz8FCRsN5G"
   },
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "    Q = sum([(prob * (mdp.get_reward(state, action, s) + gamma * state_values[s])) \n",
    "             for s, prob in mdp.get_next_states(state, action).items()])\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8rRYvrpsN5I"
   },
   "outputs": [],
   "source": [
    "test_Vs = {s : i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tEz93sasN5K"
   },
   "source": [
    "Теперь оцениваем полезность самого состоия:\n",
    "\n",
    "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QA8c-2_fsN5K"
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state): \n",
    "        return 0\n",
    "\n",
    "    V = max([sum([prob*(mdp.get_reward(state, action, s) + gamma * state_values[s]) \n",
    "                  for s, prob in mdp.get_next_states(state, action).items()])\n",
    "             for action in mdp.get_possible_actions(state)])\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ym4og12hsN5N"
   },
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emA2sv-QsN5P"
   },
   "source": [
    "Теперь создаем основной цикл итерационного оценки полезности состояний с критерием остановки, который проверяет насколько изменились оценки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "er9KwtjBsN5Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 3.50000   |   V(start): 0.000 \n",
      "iter    1   |   diff: 1.89000   |   V(start): 0.000 \n",
      "iter    2   |   diff: 1.70100   |   V(start): 1.701 \n",
      "iter    3   |   diff: 1.13542   |   V(start): 1.854 \n",
      "iter    4   |   diff: 0.73024   |   V(start): 2.584 \n",
      "iter    5   |   diff: 0.61135   |   V(start): 3.186 \n",
      "iter    6   |   diff: 0.54664   |   V(start): 3.590 \n",
      "iter    7   |   diff: 0.49198   |   V(start): 4.082 \n",
      "iter    8   |   diff: 0.42210   |   V(start): 4.463 \n",
      "iter    9   |   diff: 0.36513   |   V(start): 4.816 \n",
      "iter   10   |   diff: 0.32862   |   V(start): 5.145 \n",
      "iter   11   |   diff: 0.29262   |   V(start): 5.429 \n",
      "iter   12   |   diff: 0.26189   |   V(start): 5.691 \n",
      "iter   13   |   diff: 0.23503   |   V(start): 5.925 \n",
      "iter   14   |   diff: 0.21124   |   V(start): 6.135 \n",
      "iter   15   |   diff: 0.19012   |   V(start): 6.325 \n",
      "iter   16   |   diff: 0.17091   |   V(start): 6.496 \n",
      "iter   17   |   diff: 0.15366   |   V(start): 6.649 \n",
      "iter   18   |   diff: 0.13830   |   V(start): 6.788 \n",
      "iter   19   |   diff: 0.12445   |   V(start): 6.912 \n",
      "iter   20   |   diff: 0.11200   |   V(start): 7.024 \n",
      "iter   21   |   diff: 0.10079   |   V(start): 7.125 \n",
      "iter   22   |   diff: 0.09071   |   V(start): 7.216 \n",
      "iter   23   |   diff: 0.08164   |   V(start): 7.297 \n",
      "iter   24   |   diff: 0.07347   |   V(start): 7.371 \n",
      "iter   25   |   diff: 0.06612   |   V(start): 7.437 \n",
      "iter   26   |   diff: 0.05951   |   V(start): 7.496 \n",
      "iter   27   |   diff: 0.05356   |   V(start): 7.550 \n",
      "iter   28   |   diff: 0.04820   |   V(start): 7.598 \n",
      "iter   29   |   diff: 0.04338   |   V(start): 7.641 \n",
      "iter   30   |   diff: 0.03904   |   V(start): 7.681 \n",
      "iter   31   |   diff: 0.03514   |   V(start): 7.716 \n",
      "iter   32   |   diff: 0.03163   |   V(start): 7.747 \n",
      "iter   33   |   diff: 0.02846   |   V(start): 7.776 \n",
      "iter   34   |   diff: 0.02562   |   V(start): 7.801 \n",
      "iter   35   |   diff: 0.02306   |   V(start): 7.824 \n",
      "iter   36   |   diff: 0.02075   |   V(start): 7.845 \n",
      "iter   37   |   diff: 0.01867   |   V(start): 7.864 \n",
      "iter   38   |   diff: 0.01681   |   V(start): 7.881 \n",
      "iter   39   |   diff: 0.01513   |   V(start): 7.896 \n",
      "iter   40   |   diff: 0.01361   |   V(start): 7.909 \n",
      "iter   41   |   diff: 0.01225   |   V(start): 7.922 \n",
      "iter   42   |   diff: 0.01103   |   V(start): 7.933 \n",
      "iter   43   |   diff: 0.00992   |   V(start): 7.943 \n",
      "iter   44   |   diff: 0.00893   |   V(start): 7.952 \n",
      "iter   45   |   diff: 0.00804   |   V(start): 7.960 \n",
      "iter   46   |   diff: 0.00724   |   V(start): 7.967 \n",
      "iter   47   |   diff: 0.00651   |   V(start): 7.973 \n",
      "iter   48   |   diff: 0.00586   |   V(start): 7.979 \n",
      "iter   49   |   diff: 0.00527   |   V(start): 7.984 \n",
      "iter   50   |   diff: 0.00475   |   V(start): 7.989 \n",
      "iter   51   |   diff: 0.00427   |   V(start): 7.993 \n",
      "iter   52   |   diff: 0.00384   |   V(start): 7.997 \n",
      "iter   53   |   diff: 0.00346   |   V(start): 8.001 \n",
      "iter   54   |   diff: 0.00311   |   V(start): 8.004 \n",
      "iter   55   |   diff: 0.00280   |   V(start): 8.007 \n",
      "iter   56   |   diff: 0.00252   |   V(start): 8.009 \n",
      "iter   57   |   diff: 0.00227   |   V(start): 8.011 \n",
      "iter   58   |   diff: 0.00204   |   V(start): 8.014 \n",
      "iter   59   |   diff: 0.00184   |   V(start): 8.015 \n",
      "iter   60   |   diff: 0.00166   |   V(start): 8.017 \n",
      "iter   61   |   diff: 0.00149   |   V(start): 8.019 \n",
      "iter   62   |   diff: 0.00134   |   V(start): 8.020 \n",
      "iter   63   |   diff: 0.00121   |   V(start): 8.021 \n",
      "iter   64   |   diff: 0.00109   |   V(start): 8.022 \n",
      "iter   65   |   diff: 0.00098   |   V(start): 8.023 \n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(mdp, state_values=None, gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" performs num_iter value iteration steps starting from state_values\"\"\"\n",
    "    # initialize V(s)\n",
    "    state_values = state_values or {s : 0 for s in mdp.get_all_states()}\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
    "        new_state_values = {s : get_new_state_value(mdp, state_values, s, gamma) for s in mdp.get_all_states()}\n",
    "        \n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Compute difference\n",
    "        diff =  max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "        \n",
    "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \"%(i, diff, new_state_values[mdp._initial_state]))\n",
    "        \n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            print(\"Converged\")\n",
    "            break\n",
    "            \n",
    "    return state_values\n",
    "\n",
    "state_values = value_iteration(mdp, num_iter = 100, min_difference = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cKoN2-tsN5V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final state values: {'s1': 11.163174814980803, 's0': 8.023123818663871, 's2': 8.915559364985523}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 8.032)  < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSIBT-49sN5X"
   },
   "source": [
    "По найденным полезностям и зная модель переходов легко найти опитмальную стратегию:\n",
    "\n",
    "$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9m7NArGsN5Y"
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state): return None\n",
    "    \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    i = np.argmax([get_action_value(mdp, state_values, state, action, gamma) for action in actions])\n",
    "    \n",
    "    \n",
    "    return actions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCt9IjLnsN5Z"
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', gamma=0.9) == 'a1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-omPxnyksN5b"
   },
   "source": [
    "### Задание 2\n",
    "\n",
    "Теперь проверим работу итерации по ценностям на классической задаче FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhBAOCyEsN5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxLCrDOZsN5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 1.00000   |   V(start): 0.000 \n",
      "iter    1   |   diff: 0.90000   |   V(start): 0.000 \n",
      "iter    2   |   diff: 0.81000   |   V(start): 0.000 \n",
      "iter    3   |   diff: 0.72900   |   V(start): 0.000 \n",
      "iter    4   |   diff: 0.65610   |   V(start): 0.000 \n",
      "iter    5   |   diff: 0.59049   |   V(start): 0.590 \n",
      "iter    6   |   diff: 0.00000   |   V(start): 0.590 \n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kS3QDqNsN5h"
   },
   "source": [
    "Визуализируем нашу стратегию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRbL3izdsN5h"
   },
   "outputs": [],
   "source": [
    "def draw_policy(mdp, state_values, gamma=0.9):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    h,w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
    "    plt.imshow(V.reshape(w,h), cmap='gray', interpolation='none', clim=(0,1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h)-.5)\n",
    "    ax.set_yticks(np.arange(w)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down':(0, -1), 'right':(1,0), 'up':(-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y,x].item()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None: continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y,u*.3, -v*.3, color='m', head_width=0.1, head_length=0.1) \n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQiqx1QesN5k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 29\n",
      "iter    0   |   diff: 0.00000   |   V(start): 0.198 \n",
      "Converged\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62xktmhWsN5n"
   },
   "source": [
    "Тестируем на различных вариантах окружения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZsHbVNzsN5o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 0.80000   |   V(start): 0.000 \n",
      "iter    1   |   diff: 0.57600   |   V(start): 0.000 \n",
      "iter    2   |   diff: 0.41472   |   V(start): 0.000 \n",
      "iter    3   |   diff: 0.29860   |   V(start): 0.000 \n",
      "iter    4   |   diff: 0.24186   |   V(start): 0.000 \n",
      "iter    5   |   diff: 0.19349   |   V(start): 0.000 \n",
      "iter    6   |   diff: 0.15325   |   V(start): 0.000 \n",
      "iter    7   |   diff: 0.12288   |   V(start): 0.000 \n",
      "iter    8   |   diff: 0.09930   |   V(start): 0.000 \n",
      "iter    9   |   diff: 0.08037   |   V(start): 0.000 \n",
      "iter   10   |   diff: 0.06426   |   V(start): 0.000 \n",
      "iter   11   |   diff: 0.05129   |   V(start): 0.000 \n",
      "iter   12   |   diff: 0.04330   |   V(start): 0.000 \n",
      "iter   13   |   diff: 0.03802   |   V(start): 0.033 \n",
      "iter   14   |   diff: 0.03332   |   V(start): 0.058 \n",
      "iter   15   |   diff: 0.02910   |   V(start): 0.087 \n",
      "iter   16   |   diff: 0.01855   |   V(start): 0.106 \n",
      "iter   17   |   diff: 0.01403   |   V(start): 0.120 \n",
      "iter   18   |   diff: 0.00810   |   V(start): 0.128 \n",
      "iter   19   |   diff: 0.00555   |   V(start): 0.133 \n",
      "iter   20   |   diff: 0.00321   |   V(start): 0.137 \n",
      "iter   21   |   diff: 0.00247   |   V(start): 0.138 \n",
      "iter   22   |   diff: 0.00147   |   V(start): 0.139 \n",
      "iter   23   |   diff: 0.00104   |   V(start): 0.140 \n",
      "iter   24   |   diff: 0.00058   |   V(start): 0.140 \n",
      "iter   25   |   diff: 0.00036   |   V(start): 0.141 \n",
      "iter   26   |   diff: 0.00024   |   V(start): 0.141 \n",
      "iter   27   |   diff: 0.00018   |   V(start): 0.141 \n",
      "iter   28   |   diff: 0.00012   |   V(start): 0.141 \n",
      "iter   29   |   diff: 0.00007   |   V(start): 0.141 \n",
      "iter   30   |   diff: 0.00004   |   V(start): 0.141 \n",
      "iter   31   |   diff: 0.00003   |   V(start): 0.141 \n",
      "iter   32   |   diff: 0.00001   |   V(start): 0.141 \n",
      "iter   33   |   diff: 0.00001   |   V(start): 0.141 \n",
      "Converged\n",
      "average reward:  0.733\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdMVQ1XQsN5s"
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Теперь рассмотрим следующий алгоритм - итерации по стратегиям (PI):\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$   `// random or fixed action`\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Compute the state-value function $V^{\\pi_{n}}$\n",
    "- Using $V^{\\pi_{n}}$, compute the state-action-value function $Q^{\\pi_{n}}$\n",
    "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "PI включает в себя оценку полезности состояния как внутренний шаг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Pg53dh5sN5s"
   },
   "source": [
    "Вначале оценим полезности, используя текущую стартегию:\n",
    "\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "Мы будем пытаться найти точное решение, хотя могли использовать и предыдущий итерационный подход. Для этого будем решать систему линейных уравнений с помощью `np.linalg.solve`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Рассмотрим  для всех $s_i$ и раскроем суммы:\n",
    "<font size=\"1\">\n",
    "\\begin{align*}\n",
    "& V^{\\pi}(s_1) = P(s_1,\\pi(s_1),s_1)[ R(s_1,\\pi(s_1),s_1) + \\gamma V^{\\pi}(s_1)] + P(s_1,\\pi(s_1),s_2)[ R(s_2,\\pi(s_1),s_2) + \\gamma V^{\\pi}(s_2)] + \\dots \\\\\n",
    "& V^{\\pi}(s_2) = P(s_2,\\pi(s_2),s_1)[ R(s_2,\\pi(s_2),s_1) + \\gamma V^{\\pi}(s_1)] + P(s_2,\\pi(s_2),s_2)[ R(s_2,\\pi(s_2),s_2) + \\gamma V^{\\pi}(s_2)] + \\dots \\\\\n",
    "& \\dots \\\\\n",
    "& V^{\\pi}(s_n) = \\dots \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Посчитаем коэффициенты на примере $V^{\\pi}(s_2)$:\n",
    "$$V^{\\pi}(s_2) = P(s_2,\\pi(s_2),s_1)[ R(s_2,\\pi(s_2),s_1) + \\gamma V^{\\pi}(s_1)] + P(s_2,\\pi(s_2),s_2)[ R(s_2,\\pi(s_2),s_2) + \\gamma V^{\\pi}(s_2)] + \\dots $$\n",
    "\n",
    "\n",
    "Переносим $\\color{red}{V^{\\pi}(s_2)}$ направо:\n",
    "$$0  =  \\color{red}{-V^{\\pi}(s_2)} + P(s_2,\\pi(s_2),s_1)[ R(s_2,\\pi(s_2),s_1) + \\gamma V^{\\pi}(s_1)] + \\color{green}{P(s_2,\\pi(s_2),s_2)[ R(s_2,\\pi(s_2),s_2) + \\gamma V^{\\pi}(s_2)]} + \\dots $$\n",
    "\n",
    "Раскрываем $\\color{green}{скобки}$: \n",
    "$$0  = P(s_2,\\pi(s_2),s_1)[ R(s_2,\\pi(s_2),s_1) + \\gamma V^{\\pi}(s_1)] + \\color{green}{P(s_2,\\pi(s_2),s_2)R(s_2,\\pi(s_2),s_2) + P(s_2,\\pi(s_2),s_2)\\gamma V^{\\pi}(s_2) } \\color{red}{-V^{\\pi}(s_2)} + \\dots $$\n",
    "\n",
    "Группируем слагаемые $V^{\\pi}(s_2)$:\n",
    "$$0  = P(s_2,\\pi(s_2),s_1)[ R(s_2,\\pi(s_2),s_1) + \\gamma V^{\\pi}(s_1)] + \\color{green}{P(s_2,\\pi(s_2),s_2)R(s_2,\\pi(s_2),s_2)} + \\color{red}{\\Big(P(s_2,\\pi(s_2),s_2)\\gamma - 1\\Big) V^{\\pi}(s_2)} + \\dots $$\n",
    "Переносим  $\\color{green}{P(s_2,\\pi(s_2),s_2)R(s_2,\\pi(s_2),s_2)}$ налево:\n",
    "$$\\color{green}{-P(s_2,\\pi(s_2),s_2)R(s_2,\\pi(s_2),s_2)}  = P(s_2,\\pi(s_2),s_1)[ R(s_2,\\pi(s_2),s_1) + \\gamma V^{\\pi}(s_1)] + \\color{red}{\\Big(P(s_2,\\pi(s_2),s_2)\\gamma - 1\\Big) V^{\\pi}(s_2)} + \\dots $$\n",
    "\n",
    "Раскрываем скобки для $V^{\\pi}(s_i)$, при $i \\neq 2$: \n",
    "$$\\color{green}{-P(s_2,\\pi(s_2),s_2)R(s_2,\\pi(s_2),s_2)}  = \\color{blue}{P(s_2,\\pi(s_2),s_1)R(s_2,\\pi(s_2),s_1)} + P(s_2,\\pi(s_2),s_1)\\gamma V^{\\pi}(s_1) + \\color{red}{\\Big(P(s_2,\\pi(s_2),s_2)\\gamma - 1\\Big) V^{\\pi}(s_2)} + \\dots $$\n",
    "Переносим $\\color{blue}{P(s_2,\\pi(s_2),s_i)R(s_2,\\pi(s_2),s_i)}$ налево:\n",
    "$$\\color{green}{-P(s_2,\\pi(s_2),s_2)R(s_2,\\pi(s_2),s_2)} \\color{blue}{-P(s_2,\\pi(s_2),s_1)R(s_2,\\pi(s_2),s_1) - \\dots} =  P(s_2,\\pi(s_2),s_1)\\gamma V^{\\pi}(s_1) + \\color{red}{\\Big(P(s_2,\\pi(s_2),s_2)\\gamma - 1\\Big) V^{\\pi}(s_2)} + \\dots $$\n",
    "\n",
    "Формируем матрицу $A$, значениями которой являются коэффициенты при $\\color{red}{V^{\\pi}(s_i)}$ и $\\color{blue}{вектор}$ $b$: \n",
    "$$\\color{blue}{-P(s_2,\\pi(s_2),s_2)R(s_2,\\pi(s_2),s_2) -P(s_2,\\pi(s_2),s_1)R(s_2,\\pi(s_2),s_1) - \\dots} =  P(s_2,\\pi(s_2),s_1)\\gamma \\color{red}{V^{\\pi}(s_1)} + \\Big(P(s_2,\\pi(s_2),s_2)\\gamma - 1\\Big) \\color{red}{V^{\\pi}(s_2)} + \\dots $$\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sP2gL08osN5u"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "def compute_vpi(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Computes V^pi(s) FOR ALL STATES under given policy.\n",
    "    :param policy: a dict of currently chosen actions {s : a}\n",
    "    :returns: a dict {state : V^pi(state) for all states}\n",
    "    \"\"\"\n",
    "    states = mdp.get_all_states()\n",
    "    A, b = [], []\n",
    "    for i, state in enumerate(states):\n",
    "        if state in policy:\n",
    "            a = policy[state]\n",
    "            A.append([(1-gamma * mdp.get_transition_prob(state, a, s) if i == j else -gamma * mdp.get_transition_prob(state, a, s)) \n",
    "                  for j, s in enumerate(states)])\n",
    "            b.append(sum([prob * mdp.get_reward(state, a, next_state) \n",
    "                      for next_state, prob in mdp.get_next_states(state, a).items()]))\n",
    "        else:\n",
    "            A.append([(1 if i == j else 0) for j, s in enumerate(states)])\n",
    "            b.append(0)\n",
    "    A = np.array(A)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    values = solve(A, b)\n",
    "    \n",
    "    state_values = {states[i] : values[i] for i in range(len(states))}\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1aG9dcJVsN5w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s1': 9.267726745376374, 's0': 5.804272162697058, 's2': 7.094110421074184}\n"
     ]
    }
   ],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's1': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "gamma = 0.9            # discount for MDP\n",
    "\n",
    "test_policy = {s: np.random.choice(mdp.get_possible_actions(s)) for s in mdp.get_all_states()}\n",
    "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
    "\n",
    "print(new_vpi)\n",
    "\n",
    "assert type(new_vpi) is dict, \"compute_vpi must return a dict {state : V^pi(state) for all states}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hD-bBuwKsN5y"
   },
   "source": [
    "Теперь обновляем стратегию на основе новых значений полезностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pKbE1visN50"
   },
   "outputs": [],
   "source": [
    "def compute_new_policy(mdp, vpi, gamma):\n",
    "    \"\"\"\n",
    "    Computes new policy as argmax of state values\n",
    "    :param vpi: a dict {state : V^pi(state) for all states}\n",
    "    :returns: a dict {state : optimal action for all states}\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "    for state in mdp.get_all_states():\n",
    "        Q[state] = {}\n",
    "        for a in mdp.get_possible_actions(state):\n",
    "            values = [] \n",
    "            for next_state in mdp.get_next_states(state, a):\n",
    "                r = mdp.get_reward(state, a, next_state)\n",
    "                p = mdp.get_transition_prob(state, a, next_state)\n",
    "                values.append(p * (r + gamma * vpi[next_state]))\n",
    "            Q[state][a] = sum(values)\n",
    "    \n",
    "    policy ={}\n",
    "    for state in mdp.get_all_states():\n",
    "        actions = mdp.get_possible_actions(state)\n",
    "        if actions:\n",
    "            i = actions[0]\n",
    "            for a in actions:\n",
    "                if Q[state][a] > Q[state][i]:\n",
    "                    i = a\n",
    "            policy[state] = i\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKkZnfGRsN53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s1': 'a0', 's0': 'a1', 's2': 'a0'}\n"
     ]
    }
   ],
   "source": [
    "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
    "\n",
    "print(new_policy)\n",
    "\n",
    "assert type(new_policy) is dict, \"compute_new_policy must return a dict {state : optimal action for all states}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHax1qV5sN55"
   },
   "source": [
    "Собираем все в единый цикл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4rfeoFasN56"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, policy=None, gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" \n",
    "    Run the policy iteration loop for num_iter iterations or till difference between V(s) is below min_difference.\n",
    "    If policy is not given, initialize it at random.\n",
    "    \"\"\"\n",
    "    for i in range(num_iter):\n",
    "        if not policy:\n",
    "            policy = {}\n",
    "            for s in mdp.get_all_states():\n",
    "                if mdp.get_possible_actions(s):\n",
    "                    np.random.choice(mdp.get_possible_actions(s))\n",
    "            \n",
    "        state_values = compute_vpi(mdp, policy, gamma)\n",
    "        \n",
    "        policy = compute_new_policy(mdp, state_values, gamma)    \n",
    "        \n",
    "    \n",
    "    return state_values, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJm34Gc5sN57"
   },
   "source": [
    "Тестируем на FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NC7hXWhssN58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward:  0.888\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values, policy = policy_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(policy[s])\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnJB3YwOsN5-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sem2_dp.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
