{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение опций \n",
    "Нашей задачей будет создание набора опций, каждая из которых должна быть обучена достигать определенные состояния в задаче такси.Обученные опции затем могут быть применены для создания и обучения иерархии. Мы будем использовать QLearningAgent, которого мы написали на одном из прошлых семинаров. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# импортируем файлы и создаем окружение\n",
    "import gym\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "environment = gym.make('Taxi-v2')\n",
    "environment.render()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем классс для Q-агента\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, gamma, get_legal_actions):\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._q_values = defaultdict(lambda: defaultdict(lambda: 0))  # when called, non-existent values appear as zeros\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"\n",
    "          Returns Q(state,action)\n",
    "        \"\"\"\n",
    "        return self._q_values[state][action]\n",
    "\n",
    "    def set_q_value(self, state, action, value):\n",
    "        \"\"\"\n",
    "          Sets the Qvalue for [state,action] to the given value\n",
    "        \"\"\"\n",
    "        self._q_values[state][action] = value\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.\n",
    "        \"\"\"\n",
    "\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        value = max([self.get_q_value(state, action) for action in possible_actions])\n",
    "        return value\n",
    "\n",
    "    def get_policy(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.\n",
    "\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        best_action = None\n",
    "\n",
    "        for action in possible_actions:\n",
    "            if best_action is None:\n",
    "                best_action = action\n",
    "            elif self.get_q_value(state, action) > self.get_q_value(state, best_action):\n",
    "                best_action = action\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "          Compute the action to take in the current state, including exploration.\n",
    "\n",
    "          With probability self.epsilon, we should take a random action.\n",
    "          otherwise - the best policy action (self.getPolicy).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # если в текущей ситуации нет возможных действий - возвращаем None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = random.choice(possible_actions)\n",
    "        else:\n",
    "            action = self.get_policy(state)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, next_state, reward):\n",
    "        t = self.alpha * (reward + self.gamma * self.get_value(next_state) - self.get_q_value(state, action))\n",
    "        reference_qvalue = self.get_q_value(state, action) + t\n",
    "        self.set_q_value(state, action, reference_qvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 \n",
    "Разберемся как реализована среда Taxi: https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py\n",
    "\n",
    "Создадим 4 окружения аналогичных Taxi, в которых целью агента будет достижение одной из точек: R, G, B, Y соответственно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiStepWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env, target_id, target_reward):\n",
    "        \"\"\"\n",
    "        target_id - индекс целевой точки в окружении такси (см self.locs = locs = [(0,0), (0,4), (4,0), (4,3)])\n",
    "        target_reward - вознаграждение за достижение цели\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self._target = target_id\n",
    "        self._target_reward = target_reward\n",
    "\n",
    "    def _step(self, action):\n",
    "        # получаем изначальные параметры (state, reward, _, obs), которые передает среда, используя метод step \n",
    "        # проверяем является ли полученнуе состояние завершающим для нашего модифицированного окружения\n",
    "        # изменяем вознаграждение (reward) и флаг завершения эпизода (is_done)\n",
    "        # за каждое действие будем давать вознаграждение -1, за достижение цели - self._target_reward\n",
    "        # Ваш код здесь\n",
    "        \n",
    "        return state, reward, is_done, obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим наши среды, используя случайную стратегию.  Порядок точек должен быть  R, G, Y, B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "state:18 reward:50\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "state:87 reward:50\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "state:416 reward:50\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[43mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "state:469 reward:50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for target in range(4):\n",
    "    # создаем окружение с заданным целевым состоянием\n",
    "    # Ваш код здесь\n",
    "    \n",
    "    # применяем случаную стратегию, пока эпизод не завершится\n",
    "    # Ваш код здесь\n",
    "    \n",
    "    wrapped_env.render()\n",
    "    print(\"state:{s} reward:{r}\\n\".format(**locals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# воспользуеся реализованной на предыдущем семинаре функцией\n",
    "def play_and_train(env, agent, t_max=10 ** 4):\n",
    "    total_discounted_reward = 0.0\n",
    "    s = env.reset()\n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(s)\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        agent.update(s, a, next_s, r)\n",
    "        s = next_s\n",
    "        total_discounted_reward += r\n",
    "        if done:\n",
    "            break\n",
    "    return total_discounted_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 \n",
    "1. Обучим агентов на созданных нами окружениях.\n",
    "2. Создадим упрощенный вариант опций, каждая опция будет иметь стратегию, множество начальных и конечных состояний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = environment.action_space.n\n",
    "\n",
    "# параметры, которые будут использовать агенты\n",
    "params = {\"alpha\": 0.1, \"epsilon\": 0.1, \"gamma\": 0.99, \"get_legal_actions\": lambda s: range(4)}\n",
    "\n",
    "# создаем агентов \n",
    "agents_for_options = [QLearningAgent(**params) for _ in range(4)]\n",
    "\n",
    "\n",
    "for index in range(4):\n",
    "    # используя созданных окружения обучаем на них агентов\n",
    "    # Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализуем класс опции\n",
    "class Option:\n",
    "    def __init__(self, policy, termination, initial):\n",
    "        \"\"\"\n",
    "            policy - стратегия, у которой должны быть методы update и get_action\n",
    "            initial- множество состояний, в которых опция может быть запущена\n",
    "            termination - множество состояний, в которых опция должна быть завершена (упрощенная версия)\n",
    "        \"\"\"\n",
    "        self.policy = policy\n",
    "        self.termination_lambda = termination\n",
    "        self.initial_lambda = initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = []\n",
    "for index, agent in enumerate(agents_for_options):\n",
    "    # Испоьзуя обученных агентов создаем опции. \n",
    "    # Начальными состоянми зададим все кроме целевых\n",
    "    # Ваш код здесь \n",
    "    \n",
    "    options.append(Option(policy=agent, termination=, initial=))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "Напишем функцию, которая будет запускать опцию и возвращать дисконтированное вознаграждения, опираясь на число совершенных действий. \n",
    "$$ R = r_{1} + \\gamma r_{2} + \\gamma^{2} r_{3} + \\dots + \\gamma^{t-1}r_{t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_option(s, option, gamma, env):\n",
    "    \"\"\"\n",
    "    option - опция, стратегию которой будем использовать\n",
    "    gamma - дисконтирующий коэффициент\n",
    "    env - среда\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # Ваш код здесь\n",
    "    # используем метод get_action, стратегии нашей опции, взаимодействуем с окружением и сохраняем правильное вознаграждение\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "smdp reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v2')\n",
    "s = env.reset()\n",
    "\n",
    "env.render()\n",
    "r = go_option(s, options[0], 0.99, env)\n",
    "env.render()\n",
    "print(\"smdp reward: {r}\".format(**locals()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что все хорошо, но мы забыли рассмотреть вариант, когда пассажир может находиться в такси! Переведем среду в состояние, где пассажира мы уже подобрали и посмотрим как ведет одна из опций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "while list(env.unwrapped.decode(s))[2] != 4:\n",
    "    s, _, _, _ = env.step(random.randint(0, 5))\n",
    "    \n",
    "env.render()\n",
    "r = go_option(s, options[0], 0.99, env)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Видим, что опции не обучились действовать в такой ситуации. \n",
    "Исправим нашу функцию обучения так, чтобы опции работали корректно для всех возможных состояний среды и сгенерируем их заново."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_for_options = [QLearningAgent(**params) for _ in range(4)]\n",
    "for index in range(4):\n",
    "    pass\n",
    "    # Ваш код здесь\n",
    "    \n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим работу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B:\u001b[42m_\u001b[0m|\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "while list(env.unwrapped.decode(s))[2] != 4:\n",
    "    s, _, _, _ = env.step(random.randint(0, 5))\n",
    "    \n",
    "env.render()\n",
    "r = go_option(s, options[0], 0.99, env)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бонус \n",
    "Реализуйте иерархию, используя элементарные (опции из одного действия) и обученные опции."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
