{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 1: MDP + CrossEntropy\n",
    "### Майнор ВШЭ, 17.01.2019\n",
    "\n",
    "Обучение с подкреплением (RL) является направлением машинного обучения и изучает взаимодействие агента, которому необходимо максимизировать долговременный выигрыш в некоторой среде. Агенту не сообщается сведений о правильности действий, как в большинстве задач машинного обучения, вместо этого агент должен определить выгодные действия самостоятельно испробовав их. Испытание действий и отсроченная награда являются основными отличительными признаками RL.\n",
    "\n",
    "<img src=\"rl_simple.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rl_intro.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные составляющие модели RL:\n",
    "* $s_t$ - состояние среды в момент времени $t$,\n",
    "* $a_t$ - действие, совершаемое агентом в момент времени $t$,\n",
    "* $r_t$ - вознаграждение, получаемое агентом при совершении действия $a_t$,\n",
    "* $\\pi$ - стратегия агента - последовательность действий или план."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В простейших моделях RL среда представляется в виде марковского процесса, где функция перехода определяется как $P(s' |s,a)$, что означает вероятность оказаться в состоянии $s'$ при совершении действия $a$ в состоянии $s$. Вознаграждение теперь определяется как $r(s,a,s')$.\n",
    "\n",
    "<img src=\"mdp.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем пользоваться стандартными средами, реализованными в библиотеке OpenAI Gym (https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym interface\n",
    "\n",
    "Основные методы класса __Env__:\n",
    "* __reset()__ - инициализация окружения в начальном состоянии _возвращает первое наблюдение_,\n",
    "* __render()__ - показать текущее состояние среды,\n",
    "* __step(a)__ - выполнить действие __a__ и получить (new observation, reward, is done, info)\n",
    " * _new observation_ - новое наблюдение после выполнения действия __a__,\n",
    " * _reward_ - вознагрждение (скаляр) за выполненное действие __a__,\n",
    " * _is done_ - True, если процесс (марковский!) завершился, False иначе,\n",
    " * _info_ - дополнительная информация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs0 = env.reset()\n",
    "print(\"initial observation code:\", obs0)\n",
    "print(\"taking action 2 (right)\")\n",
    "new_obs, reward, is_done, _ = env.step(2)\n",
    "\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "Наша цель, чтобы тележка достигла флага. Модифицируйте код ниже для выполнения этого задания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TIME_LIMIT = 250\n",
    "env = gym.wrappers.TimeLimit(gym.envs.classic_control.MountainCarEnv(),\n",
    "                             max_episode_steps=TIME_LIMIT + 1)\n",
    "s = env.reset()\n",
    "actions = {'left': 0, 'stop': 1, 'right': 2}\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "for t in range(TIME_LIMIT):\n",
    "    ### ЗДЕСЬ ваш код - заставьте тележку дойти до флага! ###\n",
    "\n",
    "    #draw game image on display\n",
    "    ax.clear()\n",
    "    ax.imshow(env.render('rgb_array'))\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    if done:\n",
    "        print(\"Well done!\")\n",
    "        break\n",
    "else:    \n",
    "    print(\"Time limit exceeded. Try again.\")\n",
    "    \n",
    "env.close()\n",
    "assert s[0] > 0.47\n",
    "print(\"You solved it!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Вероятностный подход к RL\n",
    "\n",
    "Пусть наша стратегия - это вероятностное распределение:\n",
    "\n",
    "$\\pi(s,a) = P(a|s)$\n",
    "\n",
    "Рассмотрим пример с задачей Taxi [Dietterich, 2000]. Для нее мы можем считать, что наша стратегия - это двумерный массив."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "Создадим \"равномерную\" стратегию в виде двумерного массива с равномерным рапсределением по действиям и сгенерируем игровую сессию с такой стратегией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = np.array([[1./n_actions for _ in range(n_actions)] for _ in range(n_states)])\n",
    "\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(policy,t_max=10**4):\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        ### ЗДЕСЬ ваш код - нужно выбрать действие в соответсвии со стратегией ###\n",
    "        a = None\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #Record state, action and add up reward to states,actions and total_reward accordingly. \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s,a,r = generate_session(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша задача - выделить лучшие действия и состояния, т.е. такие, при которых было лучшее вознаграждение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "    \n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "    \"\"\"\n",
    "    \n",
    "    ### ЗДЕСЬ ваш код - нужно найти порог вознаграждения по известному персентилю ###\n",
    "    reward_threshold = None\n",
    "    \n",
    "    ### ЗДЕСЬ ваш код - в соответствии с найденным порогом - отобрать подходящие состояния и действия ###\n",
    "    elite_states  = None\n",
    "    elite_actions = None\n",
    "    \n",
    "    return elite_states,elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states_batch = [\n",
    "    [1,2,3],   #game1\n",
    "    [4,2,0,2], #game2\n",
    "    [3,1]      #game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0,2,4],   #game1\n",
    "    [3,2,0,1], #game2\n",
    "    [3,3]      #game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,         #game1\n",
    "    4,         #game2\n",
    "    5,         #game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(states_batch,actions_batch,rewards_batch,percentile=0)\n",
    "test_result_40 = select_elites(states_batch,actions_batch,rewards_batch,percentile=30)\n",
    "test_result_90 = select_elites(states_batch,actions_batch,rewards_batch,percentile=90)\n",
    "test_result_100 = select_elites(states_batch,actions_batch,rewards_batch,percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
    "   and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
    "        \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "        np.all(test_result_40[1] ==[3, 2, 0, 1, 3, 3]),\\\n",
    "        \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3,1]) and \\\n",
    "        np.all(test_result_90[1] == [3,3]),\\\n",
    "        \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3,1]) and\\\n",
    "       np.all(test_result_100[1] == [3,3]),\\\n",
    "        \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы хотим написать обновляющуюся стратегию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_policy(elite_states,elite_actions):\n",
    "    \"\"\"\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "    \n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    new_policy = np.zeros([n_states,n_actions])\n",
    "    \n",
    "    ### ЗДЕСЬ ваш код - обновялем стратегию - нормируем новые частоты действий и не забываем про не встречающиеся состояния ###\n",
    "    \n",
    "    for state in range(n_states):\n",
    "        new_policy[state, a] = None\n",
    "       \n",
    "    \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elite_states, elite_actions = ([1, 2, 3, 4, 2, 0, 2, 3, 1], [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
    "\n",
    "\n",
    "new_policy = update_policy(elite_states,elite_actions)\n",
    "\n",
    "assert np.isfinite(new_policy).all(), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "assert np.all(new_policy>=0), \"Your new policy can't have negative action probabilities\"\n",
    "assert np.allclose(new_policy.sum(axis=-1),1), \"Your new policy should be a valid probability distribution over actions\"\n",
    "reference_answer = np.array([\n",
    "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.5       ,  0.        ,  0.        ,  0.5       ,  0.        ],\n",
    "       [ 0.        ,  0.33333333,  0.66666667,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.5       ,  0.5       ]])\n",
    "assert np.allclose(new_policy[:4,:5],reference_answer)\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализириуем наш процесс обучения и будем измерять распределение получаемых за сессию вознаграждений "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch,log, reward_range=[-990,+10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch,percentile)\n",
    "    log.append([mean_reward,threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward,threshold))\n",
    "    plt.figure(figsize=[8,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(list(zip(*log))[0],label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1],label='Reward thresholds')\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(rewards_batch,range=reward_range);\n",
    "    plt.vlines([np.percentile(rewards_batch,percentile)],[0],[100],label=\"percentile\",color='red')\n",
    "    plt.legend(loc=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = np.ones([n_states,n_actions])/n_actions \n",
    "n_sessions = 250  #sample this many sessions\n",
    "percentile = 50  #take this percent of session with highest rewards\n",
    "learning_rate = 0.5  #add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    ### ЗДЕСЬ ваш код - генерируем n_sessions сессий ###\n",
    "    %time sessions = []\n",
    "    \n",
    "    states_batch,actions_batch,rewards_batch = zip(*sessions)\n",
    "\n",
    "    ### ЗДЕСЬ ваш код - отбираем лучшие действия и состояния ###\n",
    "    elite_states, elite_actions = None, None\n",
    "    \n",
    "    ### ЗДЕСЬ ваш код - обновляем стратегию ###\n",
    "    new_policy = None\n",
    "    \n",
    "    policy = learning_rate*new_policy + (1-learning_rate)*policy\n",
    "    \n",
    "    #display results on chart\n",
    "    show_progress(rewards_batch,log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Видим, что модель обучается. Особенность: по 50 перцентилю порог стремится к оптимальному значению +20, хотя и не превышает +10, а среднее вознаграждение сильно колеблется в районе (-80;-20). Это объясняется веротяностой природой стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "Попробуем заменить метод обновления вероятностей на нейронную сеть. Будем тестировать нашего нового агента на известной задаче перевернутого маятника с непрерывным множеством действий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\").env  #if you see \"<classname> has no attribute .env\", remove .env or update gym\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "### ЗДЕСЬ ваш код -создаем полносвязнцую сеть с двумя слоями по 20 нейронов, активация tanh ###\n",
    "agent = \n",
    "#initialize agent to the dimension of state and amount of actions\n",
    "agent.fit([env.reset()]*n_actions, range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        ### ЗДЕСЬ ваш код -предсказываем вероятности действий по сети и выбираем одно действие ###\n",
    "        probs = []\n",
    "        a = None\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sessions = 100\n",
    "percentile = 70\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    ### ЗДЕСЬ ваш код - генерируем n_sessions сессий ###\n",
    "    #sessions = [<generate a list of n_sessions new sessions>]\n",
    "        \n",
    "    states_batch,actions_batch,rewards_batch = map(np.array,zip(*sessions))\n",
    "\n",
    "    ### ЗДЕСЬ ваш код - отбираем лучшие действия и состояния ###\n",
    "    #elite_states, elite_actions = <select elite actions just like before>\n",
    "     \n",
    "    ### ЗДЕСЬ ваш код - обновляем стратегию ###\n",
    "    #<fit agent to predict elite_actions(y) from elite_states(X)>\n",
    "        \n",
    "    show_progress(rewards_batch,log,reward_range=[0,np.max(rewards_batch)])\n",
    "    \n",
    "    if np.mean(rewards_batch)> 190:\n",
    "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Доп литература\n",
    "\n",
    "* Курс от YSDA https://github.com/yandexdataschool/Practical_RL\n",
    "* Книга Sutton, Barto Reinforcement learning: An Introduction\n",
    "* Курс от Udacity - https://classroom.udacity.com/courses/ud600"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
