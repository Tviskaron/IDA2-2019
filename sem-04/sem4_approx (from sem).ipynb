{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sem4_approx.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "AQpuhSkBTKjE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Семинар 4: аппроксимация Q-функции\n",
        "\n",
        "## Майнор ВШЭ, 14.02.2019"
      ]
    },
    {
      "metadata": {
        "id": "3KgKLUiJTKjI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В этой тетрадке мы будем использовать библиотеку __tensorflow__ для обучения нейронной сети, хотя можно использвать и любую другую библиотеку. "
      ]
    },
    {
      "metadata": {
        "id": "SDsUrk4tTKjK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7IMFc1dTTKjP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Будем тестировать наши модели на классической задаче с перевернутым маятником."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "-8pb2lfxTKjR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\").env\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "# plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vX54sySbTKjV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bk3-9NN5TKjZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Глубокое Q-обучение: построение сети\n",
        "\n",
        "Так как описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
        "\n",
        "![img](https://github.com/Tviskaron/IDA2-2019/blob/master/sem-04/qlearning_scheme.png?raw=1)\n",
        "\n",
        "Для начала попробуйте использовать только полносвязные слои (__L.Dense__) и линейные активационные функции. Сигмоиды и другие функции не будут работать с ненормализованными входными данными."
      ]
    },
    {
      "metadata": {
        "id": "KHi0meFMTKja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d9db427-b07f-4b40-a9ee-9b643410def4"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.layers as L\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "keras.backend.set_session(sess)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "M6ZJDOPJTKjd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "network = keras.models.Sequential()\n",
        "\n",
        "network.add(L.InputLayer(state_dim))\n",
        "network.add(L.Dense(300, activation=\"relu\"))\n",
        "network.add(L.Dense(n_actions))\n",
        "### Ваш код здесь - строим сеть! ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kkOAJbTHTKji",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "def get_action(state, epsilon=0):\n",
        "    \"\"\"\n",
        "    sample actions with epsilon-greedy policy\n",
        "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
        "    \"\"\"\n",
        "    \n",
        "    q_values = network.predict(state[None])[0]\n",
        "    \n",
        "    ### Ваш код здесь - нужно выбрать действия e-жадно (eps-greedy)\n",
        "#     action = None\n",
        "    if epsilon < random.random():\n",
        "      action = np.argmax(q_values)\n",
        "    else:\n",
        "      action = random.choice(range(n_actions))\n",
        "    \n",
        "    return action\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zsTrcYTSTKjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "86e4215c-366f-48fd-ee6f-e094053aa4b8"
      },
      "cell_type": "code",
      "source": [
        "assert network.output_shape == (None, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
        "assert network.layers[-1].activation == keras.activations.linear, \"please make sure you predict q-values without nonlinearity\"\n",
        "\n",
        "# test epsilon-greedy exploration\n",
        "s = env.reset()\n",
        "assert np.shape(get_action(s)) == (), \"please return just one action (integer)\"\n",
        "for eps in [0., 0.1, 0.5, 1.0]:\n",
        "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
        "    best_action = state_frequencies.argmax()\n",
        "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
        "    for other_action in range(n_actions):\n",
        "        if other_action != best_action:\n",
        "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
        "    print('e=%.1f tests passed'%eps)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e=0.0 tests passed\n",
            "e=0.1 tests passed\n",
            "e=0.5 tests passed\n",
            "e=1.0 tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qGvDGntkZa3v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YyKpjfO7TKjq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q-обучение через градиентный спуск\n",
        "\n",
        "Теперь будем приближать Q-функцию агента, минимизируя TD функцию потерь:\n",
        "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2. $$\n",
        "\n",
        "Основная тонкость состоит в использовани  $Q_{-}(s',a')$. Эта таже самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. Для этого используется функция `tf.stop_gradient`."
      ]
    },
    {
      "metadata": {
        "id": "nnd2924Md24U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "KOw0pH5WTKjs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
        "states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
        "actions_ph = tf.placeholder('int32', shape=[None])\n",
        "rewards_ph = tf.placeholder('float32', shape=[None])\n",
        "next_states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
        "is_done_ph = tf.placeholder('bool', shape=[None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L6MRQ2z6TKjx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#get q-values for all actions in current states\n",
        "predicted_qvalues = network(states_ph)\n",
        "\n",
        "#select q-values for chosen actions\n",
        "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, n_actions), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T0Sb5q9uTKjz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "\n",
        "# compute q-values for all actions in next states\n",
        "### Ваш код здесь - применяем сеть для получения q-value для next_states_ph ###\n",
        "predicted_next_qvalues = network(next_states_ph)\n",
        "\n",
        "### Ваш код здесь - вычисляем V*(next_states) по предсказанным следующим q-values ###\n",
        "next_state_values = tf.reduce_max(predicted_next_qvalues, axis=1)\n",
        "\n",
        "### Ваш код здесь - вычисляем target q-values для функции потерь ###\n",
        "target_qvalues_for_actions = rewards_ph + gamma * next_state_values\n",
        "\n",
        "# at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
        "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aA15Fi_8d4hL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2. $$\n"
      ]
    },
    {
      "metadata": {
        "id": "LK9euKgoTKj2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Ваш код здесь - среднеквадратичная функция потерь stop_gradient!###\n",
        "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions))**2\n",
        "loss = tf.reduce_mean(loss)\n",
        "\n",
        "# training function that resembles agent.update(state, action, reward, next_state) from tabular agent\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5oZQ957tTKj5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"make sure you update q-values for chosen actions and not just all actions\"\n",
        "assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"make sure you don't propagate gradient w.r.t. Q_(s',a')\"\n",
        "assert predicted_next_qvalues.shape.ndims == 2, \"make sure you predicted q-values for all actions in next state\"\n",
        "assert next_state_values.shape.ndims == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "assert target_qvalues_for_actions.shape.ndims == 1, \"there's something wrong with target q-values, they must be a vector\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "adITE6BaTKj-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Playing the game"
      ]
    },
    {
      "metadata": {
        "id": "YWIDEK7ATKkA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_session(t_max=1000, epsilon=0, train=False):\n",
        "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
        "    total_reward = 0\n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        a = get_action(s, epsilon=epsilon)       \n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        \n",
        "        if train:\n",
        "            sess.run(train_step,{\n",
        "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
        "                next_states_ph: [next_s], is_done_ph: [done]\n",
        "            })\n",
        "\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if done: break\n",
        "            \n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCeeMefwTKkC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epsilon = 0.8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "-fMjfHxiTKkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "cc8bb4c7-37ff-49f8-bb38-ed132adbc730"
      },
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
        "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
        "    \n",
        "    epsilon *= 0.95\n",
        "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
        "    \n",
        "    if np.mean(session_rewards) > 300:\n",
        "        print (\"You Win!\")\n",
        "        break\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #0\tmean reward = 33.600\tepsilon = 0.800\n",
            "epoch #1\tmean reward = 45.630\tepsilon = 0.760\n",
            "epoch #2\tmean reward = 52.020\tepsilon = 0.722\n",
            "epoch #3\tmean reward = 59.880\tepsilon = 0.686\n",
            "epoch #4\tmean reward = 72.500\tepsilon = 0.652\n",
            "epoch #5\tmean reward = 90.740\tepsilon = 0.619\n",
            "epoch #6\tmean reward = 122.120\tepsilon = 0.588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FSC00yvzTKkH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Интерпретация результатов\n",
        "\n",
        "\n",
        "* __ mean reward__  - средне вознаграждеие за эпизод. в Случае корреткной реализации, этот показатель будет низким 10 эпох и только затем будет возрастать и сойдется на 50-100 шагов в зависииости от архитектуры сети.\n",
        "* Если сеть ен достигает нужных результатов к концу цикла, попробуйте увеличить число нейронов в скрытом слое или поменяйте $\\epsion$.\n",
        "* __ epsilon__ обеспечивает стремление агента исследовать среду. Можно искусственно увеличвать малые значения $\\epsilon$ при низких результатаз до 0.1 - 0.5."
      ]
    },
    {
      "metadata": {
        "id": "ov9WgCLfTKkI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Запись роликов\n",
        "\n",
        "Можно использовать `gym.wrappers.Monitor` для записи сессий агента. \n",
        "\n",
        "Для финальной пробы агента, мы будем ставить  epsilon=0."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "clvYdQwTTKkI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #record sessions\n",
        "# import gym.wrappers\n",
        "# env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
        "# sessions = [generate_session(epsilon=0, train=False) for _ in range(100)]\n",
        "# env.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rOmYAfYJTKkO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #show video\n",
        "# from IPython.display import HTML\n",
        "# import os\n",
        "\n",
        "# video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
        "\n",
        "# HTML(\"\"\"\n",
        "# <video width=\"640\" height=\"480\" controls>\n",
        "#   <source src=\"{}\" type=\"video/mp4\">\n",
        "# </video>\n",
        "# \"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GytnjQcqTKkR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}